bodyfat_orig <- read.table('BodyFat.txt', header = TRUE, row.names = 1)
bodyfat_orig <- as.data.frame(bodyfat_orig)
bodyfat <- bodyfat_orig[-39, ]
bodyfat <- bodyfat[, c(2:ncol(bodyfat), 1)]
#Of note, I got this from Chat GPT
bestAIC <- bestglm(bodyfat, IC = 'AIC')
bestBIC <- bestglm(bodyfat, IC = 'BIC')
summary(bestAIC$BestModel)
summary(bestBIC$BestModel)
int <- lm(brozek ~ 1, data = bodyfat)
all <- lm(brozek ~ ., data = bodyfat)
forw_AIC <- step(int,
direction = "forward",
scope = list(lower = int, upper = all))
forw_BIC <- step(int,
direction = "forward", k = log(nrow(bodyfat)),
scope = list(lower = int, upper = all))
summary(forw_AIC)
summary(forw_BIC)
back_AIC <- step(int,
direction = "backward",
scope = list(lower = int, upper = all))
back_BIC <- step(int,
direction = "backward", k = log(nrow(bodyfat)),
scope = list(lower = int, upper = all))
summary(back_AIC)
summary(back_BIC)
step_AIC <- step(int, direction = 'both',
scope = list(lower = int, upper = all))
step_BIC <- step(int, direction = 'both',
k = log(nrow(bodyfat)),
scope = list(lower = int, upper = all))
summary(step_AIC)
summary(step_BIC)
bf_x <- as.matrix(bodyfat[, 1:6])
bf_y <- bodyfat[, 7]
bf_lasso <- cv.glmnet(x = bf_x, y = bf_y, type.measure = 'mse', alpha = 1)
coef(bf_lasso, s = 'lambda.1se')
bf_elastic <- cv.glmnet(x = bf_x, y = bf_y, type.measure = 'mse', alpha = .5)
coef(bf_elastic, s = 'lambda.1se')
best.lm <- lm(brozek ~ weight + height + neck + abdom, data = bodyfat)
summary(best.lm)
bodyfat_new <- tibble(bodyfat, best.lm$residuals)
bodyfat_new <- subset(bodyfat_new, select = -c(age, chest))
resid_vs_weight <- ggplot(data = bodyfat_new) +
geom_point(mapping = aes(x = weight, y = best.lm$residuals)) +
theme(aspect.ratio = 1)
resid_vs_height <- ggplot(data = bodyfat_new) +
geom_point(mapping = aes(x = height, y = best.lm$residuals)) +
theme(aspect.ratio = 1)
resid_vs_neck <- ggplot(data = bodyfat_new) +
geom_point(mapping = aes(x = neck, y = best.lm$residuals)) +
theme(aspect.ratio = 1)
resid_vs_abdom <- ggplot(data = bodyfat_new) +
geom_point(mapping = aes(x = abdom, y = best.lm$residuals)) +
theme(aspect.ratio = 1)
plot_grid(resid_vs_weight, resid_vs_height, resid_vs_neck, resid_vs_abdom, ncol = 2)
bf_x <- as.matrix(bodyfat[, 1:6])
bf_y <- bodyfat[, 7]
bf_lasso <- cv.glmnet(x = bf_x, y = bf_y, type.measure = 'mse', alpha = 1)
coef(bf_lasso, s = 'lambda.1se')
avPlots(best.lm)
autoplot(best.lm, which = 1, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
ggplot(data = bodyfat) +
geom_histogram(aes(x = best.lm$residuals, y = after_stat(density)),
binwidth = 4)
autoplot(best.lm, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Wins, Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.)
# Create new model from Best Selection Model
best.lm <- lm(Quad.1.2.Win.. ~ Wins + Win...Away.from.Home + Off.Efficiency + Opp.TO.Rate + Opp.FG.. + Opp.3PT.FG., data = filtered_dat)
# Checking assumptions
# Linearity
autoplot(best.lm, which = 1, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1) +
labs(title = NULL)
# Normality
hist(best.lm$residuals, freq = FALSE, xlab = 'Residuals', main = 'Histogram of Residuals')
shapiro.test(best.lm$residuals)
# Equal Variance
autoplot(best.lm, which = 3, ncol = 1, nrow = 1)
# Influential Points
autoplot(best.lm, which = 4, ncol = 1, nrow = 1)  +
theme(aspect.ratio = 1)
# Multicollinearity
corrplot(cor(best_subset), type = 'upper')
vifs <- vif(best.lm)
max(vifs)
max(vifs)
vifs <- vif(best.lm)
mean(vifs)
# Linearity
autoplot(best.lm, which = 1, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1) +
labs(title = NULL)
# Normality
hist(best.lm$residuals, freq = FALSE, xlab = 'Residuals', main = 'Histogram of Residuals')
shapiro.test(best.lm$residuals)
# Equal Variance
autoplot(best.lm, which = 3, ncol = 1, nrow = 1)
# Influential Points
autoplot(best.lm, which = 4, ncol = 1, nrow = 1)  +
theme(aspect.ratio = 1)
# Multicollinearity
corrplot(cor(best_subset), type = 'upper')
vifs <- vif(best.lm)
mean(vifs)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Wins, Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> filter(Row_ID != 66)
library(tidyverse)
library(ggfortify)
library(car)
library(bestglm)
library(glmnet)
library(GGally)
library(corrplot)
library(cowplot)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Wins, Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> filter(Row_ID != 66)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Wins, Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> slice(-66)
library(tidyverse)
library(ggfortify)
library(car)
library(bestglm)
library(glmnet)
library(GGally)
library(corrplot)
library(cowplot)
# Read in data
raw_dat <- read.csv("C:/Users/18014/Downloads/2024_Post_MM_team_data.csv")
# Filtering out some of the columns that I won't need in order to make it easier to create linear models
filtered_dat <- select(raw_dat, -Team, -Conference, -Quad.1.2.Wins, -Quad.1.2.Losses, -Seed)
# Change the 'Conference Champion' column to a factor instead of 1s and 0s.
raw_dat$Conference.Champ <- as.factor(raw_dat$Conference.Champ)
# Use variable selection to create a smaller model and resolve multicollinearity issues
full_mod <- lm(Quad.1.2.Win.. ~ ., data = filtered_dat)
bestBIC <- bestglm(filtered_dat, IC = 'BIC')
summary(bestBIC$BestModel)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Wins, Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> slice(-66)
# Create new model from Best Selection Model
best.lm <- lm(Quad.1.2.Win.. ~ Wins + Win...Away.from.Home + Off.Efficiency + Opp.TO.Rate + Opp.FG.. + Opp.3PT.FG., data = filtered_dat)
View(best_subset)
# Create new model from Best Selection Model
best.lm <- lm(Quad.1.2.Win.. ~ Wins + Win...Away.from.Home + Off.Efficiency + Opp.TO.Rate + Opp.FG.. + Opp.3PT.FG., data = filtered_dat)
summary(best.lm)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> slice(-66)
# Create new model from Best Selection Model
best.lm <- lm(Quad.1.2.Win.. ~ Win...Away.from.Home + Off.Efficiency + Opp.TO.Rate + Opp.FG.. + Opp.3PT.FG., data = filtered_dat)
summary(best.lm)
# Linearity
autoplot(best.lm, which = 1, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1) +
labs(title = NULL)
# Normality
hist(best.lm$residuals, freq = FALSE, xlab = 'Residuals', main = 'Histogram of Residuals')
shapiro.test(best.lm$residuals)
sort(best.lm$residuals)
# Create new model from Best Selection Model
best.lm <- lm(Quad.1.2.Win.. ~ Win...Away.from.Home + Off.Efficiency + Opp.TO.Rate + Opp.FG.. + Opp.3PT.FG., data = best_subset)
summary(best.lm)
# Linearity
autoplot(best.lm, which = 1, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1) +
labs(title = NULL)
# Normality
hist(best.lm$residuals, freq = FALSE, xlab = 'Residuals', main = 'Histogram of Residuals')
shapiro.test(best.lm$residuals)
# Normality
hist(best.lm$residuals, freq = FALSE, xlab = 'Residuals', main = 'Histogram of Residuals')
shapiro.test(best.lm$residuals)
# Equal Variance
autoplot(best.lm, which = 3, ncol = 1, nrow = 1)
# Influential Points
autoplot(best.lm, which = 4, ncol = 1, nrow = 1)  +
theme(aspect.ratio = 1)
# Multicollinearity
corrplot(cor(best_subset), type = 'upper')
vifs <- vif(best.lm)
mean(vifs)
max(vifs)
# Extracting the p values below .1, or anything that is potentially significant
summary(best.lm)$coefficients[summary(best.lm)$coefficients[, 4] < 0.1, 4]
coef(best.lm)[c("Off.Efficiency", "Opp.TO.Rate")]
```{r, include = FALSE}
summary(bestBIC$BestModel)
library(tidymodels)
library(vroom)
library(embed)
library(kknn)
test <- vroom('test.csv')
setwd("~/Stat348/AmazonEmployeeAccess")
test <- vroom('test.csv')
train <- vroom('train.csv')
train$ACTION <- factor(train$ACTION)
my_recipe <- recipe(ACTION ~., data=train) |>
step_mutate_at(all_numeric(), fn = factor) |>
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) |>
step_normalize(all_numeric_predictors())
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
## KNN Analysis
rf_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) |>
set_engine('ranger') |>
set_mode('classification')
rf_wf <- workflow () |>
add_recipe(my_recipe) |>
add_model(rf_mod)
tuning_grid <- grid_regular(mtry(),
min_n(),
levels = 5)
tuning_grid <- grid_regular(mtry,
min_n,
levels = 5)
library(tidymodels)
tuning_grid <- grid_regular(mtry,
min_n,
levels = 5)
tuning_grid <- grid_regular(mtry(range = c(1, 10)),
min_n(range = c(2, 10)),
levels = 5)
folds <- vfold_cv(train, v = 5, repeats = 1)
CV_results <- rf_wf |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best <- CV_results |>
select_best()
final_wf_rf <- knn_rf |>
finalize_workflow(best) |>
fit(data = train)
final_wf_rf <- rf_wf |>
finalize_workflow(best) |>
fit(data = train)
rf_preds <- predict(final_wf_rf, new_data = test, type = 'prob')
preds_rf <- rf_preds |>
bind_cols(test) |>
rename(ACTION = .pred_1) |>
select(id, ACTION)
vroom_write(x=preds_rf, file = "./RFPreds.csv", delim=",")
setwd("~/Stat348/AmazonEmployeeAccess")
install.packages('discrim')
library(tidymodels)
library(vroom)
library(embed)
library(discrim)
test <- vroom('test.csv')
train <- vroom('train.csv')
train$ACTION <- factor(train$ACTION)
my_recipe <- recipe(ACTION ~., data=train) |>
step_mutate_at(all_numeric(), fn = factor) |>
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) |>
step_normalize(all_numeric_predictors())
prep <- prep(my_recipe)
## Naive Bayes
nb_mod <- maive_Bayes(Laplace = tune(), smoothess = tune()) |>
set_mode('classification') |>
set_engine('naivebayes')
## Naive Bayes
nb_mod <- naive_Bayes(Laplace = tune(), smoothess = tune()) |>
set_mode('classification') |>
set_engine('naivebayes')
## Naive Bayes
nb_mod <- naive_Bayes(Laplace = tune(), smoothness = tune()) |>
set_mode('classification') |>
set_engine('naivebayes')
nb_wf <- workflow() |>
add_recipe(myRecipe) |>
add_model(nb_mod)
nb_wf <- workflow() |>
add_recipe(my_recipe) |>
add_model(nb_mod)
tuning_grid <- grid_regular(Laplace,
smoothness,
levels = 5)
tuning_grid <- grid_regular(Laplace(range = c(.01, 100)),
smoothness(range = c(.01, 5)),
levels = 5)
folds <- vfold_cv(train, v = 5, repeats = 1)
CV_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
rlang::last_trace()
install.packages('naivebayes')
library(naivebayes)
CV_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best <- CV_results |>
select_best()
final_wf_nb <- nb_wf |>
finalize_workflow(best) |>
fit(data = train)
nb_preds <- predict(final_wf_nb, new_data = test, type = 'prob')
preds_nb <- nb_preds |>
bind_cols(test) |>
rename(ACTION = .pred_1) |>
select(id, ACTION)
vroom_write(x=preds_nb, file = "./NBPreds.csv", delim=",")
n <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
.5+length(n)
.5 + sum(n)
curve(dgamma(x, .5, .5))
curve(dgamma(x, .5, .5), col = 'red'. lwd = 2)
curve(dgamma(x, .5, .5), col = 'red', lwd = 2)
curve(dgamma(x, 70.5, 20.5), col = 'blue', lwd = 2)
curve(dgamma(x, .5, .5), col = 'red', lwd = 2)
curve(dgamma(x, 70.5, 20.5), col = 'blue', lwd = 2, add = TRUE)
curve(dgamma(x, 70.5, 20.5), col = 'blue', lwd = 2, add = TRUE)
curve(dgamma(x, .5, .5), col = 'red', lwd = 2)
curve(dgamma(x, .5, .5), col = 'red', lwd = 2, from = 0, to = 10)
curve(dgamma(x, 70.5, 20.5), col = 'blue', lwd = 2, add = TRUE)
curve(dgamma(x, .5, .5), col = 'red', lwd = 2, from = 0, to = 10)
curve(dgamma(x, 70.5, 20.5), col = 'blue', lwd = 2, add = TRUE)
curve(dgamma(x, .5, .5), col = 'red', lwd = 2, from = 0, to = 10)
curve(dgamma(x, 70.5, 20.5), col = 'blue', lwd = 2, add = TRUE)
legend('topleft', legend = c('Prior', 'Posterior'), col = c('red', 'blue'), lwd = 2)
curve(dgamma(x, .5, .5), col = 'red', lwd = 2, from = 0, to = 10)
curve(dgamma(x, 70.5, 20.5), col = 'blue', lwd = 2, add = TRUE)
legend('topright', legend = c('Prior', 'Posterior'), col = c('red', 'blue'), lwd = 2)
curve(dgamma(x, .5, rate = .5), col = 'red', lwd = 2, from = 0, to = 10)
curve(dgamma(x, 70.5, rate = 20.5), col = 'blue', lwd = 2, add = TRUE)
legend('topright', legend = c('Prior', 'Posterior'), col = c('red', 'blue'), lwd = 2)
qgamma(.025, 70.5, 20.5)
qgamma(.975, 70.5, 20.5)
qgamma(.025, 70.5, 20.5)
pgamma(4, 70.5, 20.5, lower.tail = FALSE)
ppois(4, 3.439, lower.tail = FALSE)
8/1.5
8*1.5
8*2.25
8/2.25
sum(w)
w <- c(12, 9, 10, 8, 9, 4, 10, 15, 3, 5, 11, 8, 9, 4, 2, 7, 9, 5, 4, 2, 3, 12, 10, 2, 9, 8, 13, 9, 7, 6, 6, 2, 2, 6, 8)
m <- c(2, 3, 0, 4, 1, 1, 1, 2, 2, 2, 0, 3, 2)
sum(w)
1.5 + length(w)
8 + sum(m)
1.5 + length(m)
```
1.5 + length(m)
qgamma(.025, 249, 36.5)
qgamma(.975, 249, 36.5)
qgamma(.975, 31, 14.5)
qgamma(.025, 31, 14.5)
lambda_w <- rgamma(1000, 249, 36.5)
lambda_m <- rgamma(1000, 31, 14.5)
diff <- lambda_w - lambda_m
lambda_w <- rgamma(1000, 249, 36.5)
lambda_m <- rgamma(1000, 31, 14.5)
diff <- lambda_w - lambda_m
plot(density(diff), xlab=expression(lambda[women] - lambda[men]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[women]-lambda[men])), cex.axis=cex.plots, cex.lab=cex.plots, lwd=2)
plot(density(diff), xlab=expression(lambda[women] - lambda[men]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[women]-lambda[men])), lwd=2)
mean(diff)
8 + sum(w)
1.5 + length(w)
qgamma(.025, 257, 36.5)
qgamma(.975, 257, 36.5)
pnbinom(4, 70.5, 20.5/21.5, lower.tail = FALSE)
ppois(4, 3.439, lower.tail = FALSE)
pnbinom(4, 70.5, 20.5/21.5, lower.tail = FALSE)
ppois(4, 3.439, lower.tail = FALSE)
# ppois(4, 3.439, lower.tail = FALSE)
pnbinom(4, 70.5, 20.5/21.5, lower.tail = FALSE)
qgamma(.025, 70.5, 20.5)
qgamma(.975, 70.5, 20.5)
lambda_w <- rgamma(1000, 249, 36.5)
lambda_m <- rgamma(1000, 31, 14.5)
diff <- lambda_w - lambda_m
plot(density(diff), xlab=expression(lambda[women] - lambda[men]), ylab="density", main=expression(paste("Posterior Distribution of ", lambda[women]-lambda[men])), lwd=2)
abline(v=0, lty=2)
mean(diff)
test <- vroom('test.csv')
train <- vroom('train.csv')
train$ACTION <- factor(train$ACTION)
my_recipe <- recipe(ACTION ~., data=train) |>
step_mutate_at(all_numeric(), fn = factor) |>
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) |>
step_normalize(all_numeric_predictors()) |>
step_pca(all_predictors(), threshold = .8)
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
## KNN Analysis
knn_mod <- nearest_neighbor(neighbors = tune()) |>
set_mode('classification') |>
set_engine('kknn')
knn_wf <- workflow () |>
add_recipe(my_recipe) |>
add_model(knn_mod)
tuning_grid <- grid_regular(neighbors(),
levels = 5)
folds <- vfold_cv(train, v = 5, repeats = 1)
CV_results <- knn_wf |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best <- CV_results |>
select_best()
final_wf_knn <- knn_wf |>
finalize_workflow(best) |>
fit(data = train)
knn_preds <- predict(final_wf_knn, new_data = test, type = 'prob')
preds_knn <- knn_preds |>
bind_cols(test) |>
rename(ACTION = .pred_1) |>
select(id, ACTION)
vroom_write(x=preds_knn, file = "./KNNPredsPCA.csv", delim=",")
## Logistic Regression
log_reg_mod <- logistic_reg() |>
set_engine('glm')
workflow_log <- workflow() |>
add_recipe(my_recipe) |>
add_model(log_reg_mod) |>
fit(data = train)
log_reg_pred <- predict(workflow_log, new_data = test, type = 'prob')
preds_logistic <- log_reg_pred |>
bind_cols(test) |>
rename(ACTION = .pred_1) |>
select(id, ACTION)
vroom_write(x=preds_logistic, file = "./LogisticPredsPCA.csv", delim=",")
## Penalized Regression
pen_mod <- logistic_reg(mixture = tune(), penalty = tune()) |>
set_engine('glmnet')
pen_workflow <- workflow() |>
add_recipe(my_recipe) |>
add_model(pen_mod)
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
folds <- vfold_cv(train, v = 5, repeats = 1)
CV_results <- pen_workflow |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best <- CV_results |>
select_best()
final_wf_pen <- pen_workflow |>
finalize_workflow(best) |>
fit(data = train)
pen_pred <- final_wf_pen |> predict(new_data = test, type = 'prob')
preds_penalized <- pen_pred |>
bind_cols(test) |>
rename(ACTION = .pred_1) |>
select(id, ACTION)
vroom_write(x=preds_penalized, file = "./PenalizedLogisticPredsPCA.csv", delim=",")
## KNN Analysis
rf_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 1000) |>
set_engine('ranger') |>
set_mode('classification')
rf_wf <- workflow () |>
add_recipe(my_recipe) |>
add_model(rf_mod)
tuning_grid <- grid_regular(mtry(range = c(1, 10)),
min_n(range = c(2, 10)),
levels = 5)
folds <- vfold_cv(train, v = 5, repeats = 1)
CV_results <- rf_wf |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best <- CV_results |>
select_best()
final_wf_rf <- rf_wf |>
finalize_workflow(best) |>
fit(data = train)
rf_preds <- predict(final_wf_rf, new_data = test, type = 'prob')
preds_rf <- rf_preds |>
bind_cols(test) |>
rename(ACTION = .pred_1) |>
select(id, ACTION)
vroom_write(x=preds_rf, file = "./RFPredsPCA.csv", delim=",")
## Naive Bayes
nb_mod <- naive_Bayes(Laplace = tune(), smoothness = tune()) |>
set_mode('classification') |>
set_engine('naivebayes')
nb_wf <- workflow() |>
add_recipe(my_recipe) |>
add_model(nb_mod)
tuning_grid <- grid_regular(Laplace(range = c(.01, 100)),
smoothness(range = c(.01, 5)),
levels = 5)
folds <- vfold_cv(train, v = 5, repeats = 1)
CV_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best <- CV_results |>
select_best()
final_wf_rf <- rf_wf |>
finalize_workflow(best) |>
fit(data = train)
best <- CV_results |>
select_best()
final_wf_nb <- nb_wf |>
finalize_workflow(best) |>
fit(data = train)
nb_preds <- predict(final_wf_nb, new_data = test, type = 'prob')
preds_nb <- nb_preds |>
bind_cols(test) |>
rename(ACTION = .pred_1) |>
select(id, ACTION)
vroom_write(x=preds_nb, file = "./NBPredsPCA.csv", delim=",")
